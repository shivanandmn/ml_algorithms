Weighted K-Nearest Neighbors (Weighted KNN) is an enhancement of the basic KNN algorithm. It addresses one of the fundamental limitations of standard KNN: the equal consideration given to each of the `k` neighbors in the voting process. In scenarios where neighbors vary significantly in their relevance or similarity to the query point, treating all neighbors equally can lead to suboptimal predictions. Weighted KNN resolves this by assigning different weights to the neighbors based on their distance from the query point.

### Conceptual Overview

1. **Basic Premise**: Similar to standard KNN, Weighted KNN involves identifying the `k` nearest neighbors to a query point. However, instead of giving each neighbor an equal vote, Weighted KNN assigns a weight to each neighbor, usually inversely proportional to their distance from the query point.

2. **Weighting Function**: The most common weighting strategy is to give higher weight to closer neighbors. A typical function used is the inverse of distance. For a neighbor at distance `d`, its weight might be `1/d` or `1/d^2`. The exact choice of the weighting function can vary based on the specific requirements of the application.

3. **Decision Rule**: 
   - In classification, instead of a simple majority vote, the class of each of the `k` neighbors is weighted by its assigned weight. The query point is assigned to the class with the highest total weight.
   - In regression, the predicted value is a weighted average of the values of the `k` neighbors, again using the weights based on distances.

4. **Normalization**: Weights are typically normalized so that their sum equals 1. This normalization ensures that the scale of the weights is consistent.

### Python Implementation

Here's a basic Python implementation of Weighted KNN for classification:

```python
import numpy as np
from collections import Counter

def weighted_knn_predict(X_train, y_train, query_point, k, distance_weighted=True):
    distances = np.sqrt(np.sum((X_train - query_point) ** 2, axis=1))
    k_indices = np.argsort(distances)[:k]
    k_nearest_labels = y_train[k_indices]

    if distance_weighted:
        # Inverse distance weighting
        weights = 1 / distances[k_indices]
        weighted_votes = Counter()
        for label, weight in zip(k_nearest_labels, weights):
            weighted_votes[label] += weight
        return weighted_votes.most_common(1)[0][0]
    else:
        # Standard KNN
        return Counter(k_nearest_labels).most_common(1)[0][0]

# Example Usage
X_train = np.array([[1, 2], [2, 3], [3, 4]])
y_train = np.array([0, 1, 1])
query_point = np.array([2, 2.5])

print(weighted_knn_predict(X_train, y_train, query_point, k=2))
```

### Applications and Considerations

- **Improved Performance in Certain Contexts**: Weighted KNN can outperform standard KNN in cases where the relevance of neighbors is not uniform. For example, in spatial data analysis, closer points might be significantly more relevant than farther ones.
- **Parameter Tuning**: The choice of `k` and the weighting function requires careful tuning. Cross-validation can be used to find the optimal parameters.
- **Robustness to Noise**: By weighting neighbors, the algorithm can become more robust to noise and outliers, as distant (and potentially irrelevant) points have less influence.

In summary, Weighted KNN provides a more nuanced approach to the KNN algorithm, allowing for greater flexibility and potentially higher accuracy by acknowledging that not all neighbors contribute equally to the prediction. This approach is particularly beneficial in cases where the assumption of equal relevance of all neighbors is questionable.


Example 2: Medical Diagnosis System
Context: Consider a KNN-based system for diagnosing illnesses based on patient symptoms, where the goal is to identify the illness based on the most similar cases.

Simple Inverse Distance: Might be preferable in a medical diagnosis context. It's important that the algorithm doesn't overly emphasize the nearest neighbors since a slightly farther neighbor could still be highly relevant to the diagnosis.
Squared Inverse Distance: Could overemphasize the nearest neighbors and potentially overlook slightly more distant but relevant cases, leading to a misdiagnosis.