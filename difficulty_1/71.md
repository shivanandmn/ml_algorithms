Building a k-Nearest Neighbors (KNN) algorithm from scratch for time series data involves several key steps. Since time series data have their own specific characteristics, it's crucial to tailor the KNN algorithm accordingly. Here's a step-by-step guide:

1. **Distance Metric**:
   - Choose an appropriate distance metric. For time series, Dynamic Time Warping (DTW) is often more effective than Euclidean distance as it can align sequences that may have similar patterns but are out of phase.
   - Implement the DTW algorithm. DTW calculates the optimal alignment between two time series by minimizing the cumulative distance between them.

2. **Data Representation**:
   - Represent your time series in a suitable form for KNN. This can involve normalizing the data, extracting relevant features, or using embedding techniques to capture the temporal structure.

3. **Building the KNN Algorithm**:
   - The core of the KNN algorithm involves finding the 'k' nearest neighbors of a query point based on the chosen distance metric.
   - Store your training data in a suitable data structure. For simplicity, you can use a list or an array.

4. **Prediction**:
   - For a given query time series, calculate the distance to all points in your training set.
   - Identify the 'k' closest points and perform a majority vote among these points to predict the class (for classification) or calculate the average (for regression).

5. **Evaluation**:
   - Implement a method to evaluate your model, such as cross-validation. This will help in tuning hyperparameters like 'k' and assessing the model's performance.

6. **Optimization (Optional)**:
   - For large datasets, consider optimizing the algorithm. Techniques like k-d trees, however, are less effective for high-dimensional data like time series.

Letâ€™s implement a basic version of KNN for time series classification in Python:

```python
import numpy as np

def dtw_distance(series1, series2):
    """
    Compute Dynamic Time Warping distance between two time series.
    """
    # Create distance matrix
    dtw = np.zeros((len(series1)+1, len(series2)+1))
    dtw.fill(np.inf)
    dtw[0, 0] = 0

    for i in range(1, len(series1) + 1):
        for j in range(1, len(series2) + 1):
            cost = abs(series1[i-1] - series2[j-1])
            dtw[i, j] = cost + min(dtw[i-1, j], dtw[i, j-1], dtw[i-1, j-1])

    return dtw[len(series1), len(series2)]

class TimeSeriesKNN:
    def __init__(self, k=3):
        self.k = k
        self.X_train = None
        self.y_train = None

    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train

    def predict(self, X_test):
        predictions = []
        for series in X_test:
            # Compute distances to all points in the training set
            distances = [dtw_distance(series, train_series) for train_series in self.X_train]
            
            # Sort distances and get top k
            k_indices = np.argsort(distances)[:self.k]
            k_nearest_labels = [self.y_train[i] for i in k_indices]
            
            # Majority vote
            prediction = max(set(k_nearest_labels), key=k_nearest_labels.count)
            predictions.append(prediction)

        return predictions

# Example usage:
# X_train, y_train = ... # Load or generate your training data
# X_test, y_test = ... # Load or generate your test data

# model = TimeSeriesKNN(k=3)
# model.fit(X_train, y_train)
# predictions = model.predict(X_test)
```

This is a basic implementation. In practice, you may need to add error handling, optimize performance, and possibly extend the functionality to handle different types of time series data. Remember, the effectiveness of KNN in time series analysis heavily relies on the choice of distance metric and the representation of the time series.